<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.4.0 for Hugo" />
  

  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Aryan Deshwal" />

  
  
  
    
  
  <meta name="description" content="" />

  
  <link rel="alternate" hreflang="en-us" href="https://bayesopt-tutorial.github.io/references/" />

  
  
  
    <meta name="theme-color" content="#bbdefb" />
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css" media="print" onload="this.media='all'">

  
  
  
    
    

    
    
    
    
      
      
    
    
    

    
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.19a96669ea8db3907ce02c5e3301f8b6.css" />

  



  


  


  




  
  
  

  
    <link rel="alternate" href="/references/index.xml" type="application/rss+xml" title="BO tutorial" />
  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://bayesopt-tutorial.github.io/references/" />

  
  
  
  
  
  
  
  
    
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="BO tutorial" />
  <meta property="og:url" content="https://bayesopt-tutorial.github.io/references/" />
  <meta property="og:title" content="List of references | BO tutorial" />
  <meta property="og:description" content="" /><meta property="og:image" content="https://bayesopt-tutorial.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://bayesopt-tutorial.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
  

  



  

  





  <title>List of references | BO tutorial</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper  dark " data-wc-page-id="502b26bf8b307ac38ebd0b3d7a26c333" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.d333d96014f7bcfa311294a92ace0d60.js"></script>

  




  <div class="page-header">
    












<header class="header--fixed">
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">BO tutorial</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">BO tutorial</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/syllabus"><span>Tutorial Syllabus and Slides</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link  active" href="/references"><span>List of References</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/tutorialspeakers"><span>Speakers</span></a>
          </li>

          
          

          

          
          
          
            
              
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="https://aaai.org/Conferences/AAAI-22/" target="_blank" rel="noopener"><span>AAAI 2022</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        

        
        
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    












  

  
  
  
    
  
<div class="universal-wrapper pt-3">
  <h1>List of references</h1>

  

  
</div>



<div class="universal-wrapper">
  
    <div class="article-style"><h3 id="general-introduction-and-survey">General introduction and survey</h3>
<ul>
<li>Garnett, R. (2022).  <em>Bayesian Optimization</em>. Cambridge University Press. <a href="https://bayesoptbook.com/" target="_blank" rel="noopener">Link</a></li>
<li>Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., &amp; De Freitas, N. (2015). Taking the human out of the loop: A review of Bayesian optimization. <em>Proceedings of the IEEE</em>, <em>104</em>(1), 148-175. <a href="https://ieeexplore.ieee.org/document/7352306" target="_blank" rel="noopener">Link</a></li>
<li>Frazier, P. I. (2018). A tutorial on Bayesian optimization. <em>arXiv preprint arXiv:1807.02811</em>. <a href="https://arxiv.org/abs/1807.02811" target="_blank" rel="noopener">Link</a></li>
<li>Greenhill, S., Rana, S., Gupta, S., Vellanki, P., &amp; Venkatesh, S. (2020). Bayesian optimization for adaptive experimental design: A review. <em>IEEE access</em>, <em>8</em>, 13937-13948. <a href="https://ieeexplore.ieee.org/abstract/document/8957442" target="_blank" rel="noopener">Link</a></li>
<li>Brochu, E., Cora, V. M., &amp; De Freitas, N. (2010). A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. <em>arXiv preprint arXiv:1012.2599</em>. <a href="https://arxiv.org/abs/1012.2599" target="_blank" rel="noopener">Link</a></li>
</ul>
<h3 id="gaussian-processes">Gaussian Processes</h3>
<ul>
<li>Williams, C. K., &amp; Rasmussen, C. E. (2006). <em>Gaussian processes for machine learning</em> (Vol. 2, No. 3, p. 4). Cambridge, MA: MIT press. <a href="http://www.gaussianprocess.org/gpml/" target="_blank" rel="noopener">Link</a></li>
<li>Leibfried, F., Dutordoir, V., John, S. T., &amp; Durrande, N. (2020). A tutorial on sparse Gaussian processes and variational inference. <em>arXiv preprint arXiv:2012.13962</em>. <a href="https://arxiv.org/abs/2012.13962" target="_blank" rel="noopener">Link</a></li>
</ul>
<h3 id="acquisition-functions-and-their-optimization">Acquisition functions and their optimization</h3>
<ul>
<li>Močkus, J. (1975). On Bayesian methods for seeking the extremum. In <em>Optimization techniques IFIP technical conference</em> (pp. 400-404). Springer, Berlin, Heidelberg.</li>
<li>Srinivas, N., Krause, A., Kakade, S. M., &amp; Seeger, M. (2009). Gaussian process optimization in the bandit setting: No regret and experimental design. <em>arXiv preprint arXiv:0912.3995</em>.</li>
<li>Frazier, P., Powell, W., &amp; Dayanik, S. (2009). The knowledge-gradient policy for correlated normal beliefs. <em>INFORMS journal on Computing</em>, <em>21</em>(4), 599-613.</li>
<li>Hennig, P., &amp; Schuler, C. J. Entropy search for information-efficient global optimization. Journal of Machine Learning Research (<em>JMLR</em>), 13, 1809–1837, 2012.</li>
<li>Hernandez-Lobato, J. M., Hoffman, M. W., &amp; Ghahramani, Z. Predictive entropy search for efficient global optimization of black-box functions. Advances in Neural Information Processing Systems (<em>NIPS</em>), pp. 918–926, 2014.</li>
<li>Wang, Z., &amp; Jegelka, S. (2017, July). Max-value entropy search for efficient Bayesian optimization. In <em>International Conference on Machine Learning</em> (pp. 3627-3635). PMLR.</li>
<li>Jiang, S., Jiang, D., Balandat, M., Karrer, B., Gardner, J., &amp; Garnett, R. (2020). Efficient nonmyopic bayesian optimization via one-shot multi-step trees. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 18039-18049.</li>
<li>Lam, R., Willcox, K., &amp; Wolpert, D. H. (2016). Bayesian optimization with a finite budget: An approximate dynamic programming approach. <em>Advances in Neural Information Processing Systems</em>, <em>29</em>.</li>
<li>González, J., Osborne, M., &amp; Lawrence, N. (2016, May). GLASSES: Relieving the myopia of Bayesian optimisation. In <em>Artificial Intelligence and Statistics</em> (pp. 790-799). PMLR.</li>
<li>Balandat, M., Karrer, B., Jiang, D., Daulton, S., Letham, B., Wilson, A. G., &amp; Bakshy, E. (2020). BoTorch: a framework for efficient Monte-Carlo Bayesian optimization. <em>Advances in neural information processing systems</em>, <em>33</em>, 21524-21538.</li>
<li>Kim, J., &amp; Choi, S. (2020, September). On local optimizers of acquisition functions in bayesian optimization. In <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em> (pp. 675-690). Springer, Cham.</li>
<li>Wilson, J., Hutter, F., &amp; Deisenroth, M. (2018). Maximizing acquisition functions for Bayesian optimization. <em>Advances in neural information processing systems</em>, <em>31</em>.</li>
<li>Grosnit, A., Cowen-Rivers, A. I., Tutunov, R., Griffiths, R. R., Wang, J., &amp; Bou-Ammar, H. (2021). Are we forgetting about compositional optimisers in bayesian optimisation?. Journal of Machine Learning Research, 22(160), 1-78.</li>
</ul>
<h3 id="multi-fidelity-bayesian-optimization">Multi-fidelity Bayesian optimization</h3>
<ul>
<li>Huang, D., Allen, T. T., Notz, W. I., &amp; Miller, R. A. (2006). Sequential kriging optimization using multiple-fidelity evaluations. <em>Structural and Multidisciplinary Optimization</em>, <em>32</em>(5), 369-382.</li>
<li>Swersky, K., Snoek, J., &amp; Adams, R. P. (2013). Multi-task bayesian optimization. <em>Advances in neural information processing systems</em>, <em>26</em>.</li>
<li>Kandasamy, K., Dasarathy, G., Schneider, J., &amp; Póczos, B. (2017, July). Multi-fidelity bayesian optimisation with continuous approximations. In <em>International Conference on Machine Learning</em> (pp. 1799-1808). PMLR.</li>
<li>Klein, A., Falkner, S., Bartels, S., Hennig, P., &amp; Hutter, F. (2017, April). Fast bayesian optimization of machine learning hyperparameters on large datasets. In <em>Artificial intelligence and statistics</em> (pp. 528-536). PMLR.</li>
<li>Song, J., Chen, Y., &amp; Yue, Y. (2019, April). A general framework for multi-fidelity bayesian optimization with gaussian processes. In <em>The 22nd International Conference on Artificial Intelligence and Statistics</em> (pp. 3158-3167). PMLR.</li>
<li>Wu, J., Toscano-Palmerin, S., Frazier, P. I., &amp; Wilson, A. G. (2020, August). Practical multi-fidelity Bayesian optimization for hyperparameter tuning. In <em>Uncertainty in Artificial Intelligence</em> (pp. 788-798). PMLR.</li>
<li>Takeno, S., Fukuoka, H., Tsukada, Y., Koyama, T., Shiga, M., Takeuchi, I., &amp; Karasuyama, M. (2020, November). Multi-fidelity Bayesian optimization with max-value entropy search and its parallelization. In <em>International Conference on Machine Learning</em> (pp. 9334-9345). PMLR.</li>
<li>Zhang, Y., Dai, Z., &amp; Low, B. K. H. (2020, August). Bayesian optimization with binary auxiliary information. In <em>Uncertainty in Artificial Intelligence</em> (pp. 1222-1232). PMLR.</li>
<li>Li, S., Xing, W., Kirby, R., &amp; Zhe, S. (2020). Multi-fidelity Bayesian optimization via deep neural networks. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 8521-8531.</li>
</ul>
<h3 id="combinatorial-bayesian-optimization">Combinatorial Bayesian optimization</h3>
<ul>
<li>Deshwal, A., Belakaria, S., Doppa, J. R., &amp; Fern, A. (2020). Optimizing discrete spaces via expensive evaluations: A learning to search framework. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> (Vol. 34, No. 04, pp. 3773-3780).</li>
<li>Deshwal, A., Belakaria, S., &amp; Doppa, J. R. (2021). Mercer features for efficient combinatorial Bayesian optimization. In <em>Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI)</em> (pp. 7210-7218).</li>
<li>Deshwal, A., &amp; Doppa, J. (2021). Combining Latent Space and Structured Kernels for Bayesian Optimization over Combinatorial Spaces. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>.</li>
<li>Deshwal, A., Belakaria, S., &amp; Doppa, J. R. (2020). Scalable combinatorial Bayesian optimization with tractable statistical models. <em>arXiv preprint arXiv:2008.08177</em>.</li>
<li>Baptista, R., &amp; Poloczek, M. (2018, July). Bayesian optimization of combinatorial structures. In <em>International Conference on Machine Learning</em> (pp. 462-471). PMLR.</li>
<li>Garrido-Merchán, E. C., &amp; Hernández-Lobato, D. (2020). Dealing with categorical and integer-valued variables in bayesian optimization with gaussian processes. <em>Neurocomputing</em>, <em>380</em>, 20-35.</li>
<li>Oh, C., Tomczak, J., Gavves, E., &amp; Welling, M. (2019). Combinatorial bayesian optimization using the graph cartesian product. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</li>
<li>Swersky, K., Rubanova, Y., Dohan, D., &amp; Murphy, K. (2020). Amortized bayesian optimization over discrete spaces. In <em>Conference on Uncertainty in Artificial Intelligence</em> (pp. 769-778). PMLR.</li>
<li>Moss, H., Leslie, D., Beck, D., Gonzalez, J., &amp; Rayson, P. (2020). Boss: Bayesian optimization over string spaces. <em>Advances in neural information processing systems</em>, <em>33</em>, 15476-15486.</li>
<li>Buathong, P., Ginsbourger, D., &amp; Krityakierne, T. (2020). Kernels over sets of finite sets using RKHS embeddings, with application to Bayesian (combinatorial) optimization. In <em>International Conference on Artificial Intelligence and Statistics</em> (pp. 2731-2741). PMLR.</li>
<li>Dadkhahi, H., Shanmugam, K., Rios, J., Das, P., Hoffman, S. C., Loeffler, T. D., &amp; Sankaranarayanan, S. (2020, August). Combinatorial black-box optimization with expert advice. In <em>Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em> (pp. 1918-1927).</li>
<li>Kim, J., McCourt, M., You, T., Kim, S., &amp; Choi, S. (2021). Bayesian optimization with approximate set kernels. <em>Machine Learning</em>, <em>110</em>(5), 857-879.</li>
<li>Griffiths, R. R., &amp; Hernández-Lobato, J. M. (2020). Constrained Bayesian optimization for automatic chemical design using variational autoencoders. Chemical science, 11(2), 577-586.</li>
</ul>
<h3 id="bo-over-hybrid-spaces">BO over hybrid spaces</h3>
<ul>
<li>Deshwal, A., Belakaria, S., &amp; Doppa, J. R. (2021). Bayesian optimization over hybrid spaces. In <em>International Conference on Machine Learning</em> (pp. 2632-2643). PMLR.</li>
<li>Daxberger, E., Makarova, A., Turchetta, M., &amp; Krause, A. (2019). Mixed-variable bayesian optimization. <em>arXiv preprint arXiv:1907.01329</em>.</li>
<li>Oh, C., Gavves, E., &amp; Welling, M. (2021). Mixed variable Bayesian optimization with frequency modulated kernels. In <em>Uncertainty in Artificial Intelligence</em> (pp. 950-960). PMLR.</li>
<li>Ru, B., Alvi, A., Nguyen, V., Osborne, M. A., &amp; Roberts, S. (2020, November). Bayesian optimisation over multiple continuous and categorical inputs. In International Conference on Machine Learning (pp. 8276-8285). PMLR.</li>
</ul>
<h3 id="high-dimensional-bayesian-optimization">High-dimensional Bayesian optimization</h3>
<ul>
<li>Eriksson, D., Pearce, M., Gardner, J., Turner, R. D., &amp; Poloczek, M. (2019). Scalable global optimization via local bayesian optimization. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</li>
<li>Li, C., Gupta, S., Rana, S., Nguyen, V., Venkatesh, S., &amp; Shilton, A. (2017). High Dimensional Bayesian Optimization Using Dropout. <em>Proceedings of the 26th International Joint Conference on Artificial Intelligence</em>, 2096–2102. Melbourne, Australia: AAAI Press.</li>
<li>Mutny, M., &amp; Krause, A. (2018). Efficient high dimensional bayesian optimization with additivity and quadrature fourier features. <em>Advances in Neural Information Processing Systems</em>, <em>31</em>.</li>
<li>Wang, Z., Gehring, C., Kohli, P., &amp; Jegelka, S. (2018, March). Batched large-scale Bayesian optimization in high-dimensional spaces. In <em>International Conference on Artificial Intelligence and Statistics</em> (pp. 745-754). PMLR.</li>
<li>Oh, C., Gavves, E., &amp; Welling, M. (2018, July). BOCK: Bayesian optimization with cylindrical kernels. In <em>International Conference on Machine Learning</em> (pp. 3868-3877). PMLR.</li>
<li>Rolland, P., Scarlett, J., Bogunovic, I., &amp; Cevher, V. (2018, March). High-dimensional Bayesian optimization via additive models with overlapping groups. In <em>International conference on artificial intelligence and statistics</em> (pp. 298-307). PMLR.</li>
<li>Li, C. L., Kandasamy, K., Póczos, B., &amp; Schneider, J. (2016, May). High dimensional Bayesian optimization via restricted projection pursuit models. In <em>Artificial Intelligence and Statistics</em> (pp. 884-892). PMLR.</li>
<li>Zhang, M., Li, H., &amp; Su, S. (2019). High dimensional Bayesian optimization via supervised dimension reduction. <em>arXiv preprint arXiv:1907.08953</em>.</li>
<li>Hoang, T. N., Hoang, Q. M., Ouyang, R., &amp; Low, K. H. (2018, April). Decentralized high-dimensional Bayesian optimization with factor graphs. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> (Vol. 32, No. 1).</li>
<li>Maddox, W. J., Balandat, M., Wilson, A. G., &amp; Bakshy, E. (2021). Bayesian optimization with high-dimensional outputs. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>.</li>
<li>Wan, X., Nguyen, V., Ha, H., Ru, B., Lu, C., &amp; Osborne, M. A. (2021). Think global and act local: Bayesian optimisation over high-dimensional categorical and mixed search spaces. <em>arXiv preprint arXiv:2102.07188</em>.</li>
</ul>
<h3 id="batch-bayesian-optimization">Batch Bayesian Optimization</h3>
<ul>
<li>Wu, J., &amp; Frazier, P. (2016). The parallel knowledge gradient method for batch Bayesian optimization. <em>Advances in neural information processing systems</em>, <em>29</em>.</li>
<li>Ginsbourger, D., Riche, R. L., &amp; Carraro, L. (2010). Kriging is well-suited to parallelize optimization. In <em>Computational intelligence in expensive optimization problems</em> (pp. 131-162). Springer, Berlin, Heidelberg.</li>
<li>González, J., Dai, Z., Hennig, P., &amp; Lawrence, N. (2016, May). Batch Bayesian optimization via local penalization. In <em>Artificial intelligence and statistics</em> (pp. 648-657). PMLR.</li>
<li>Nguyen, V., Rana, S., Gupta, S. K., Li, C., &amp; Venkatesh, S. (2016, December). Budgeted batch Bayesian optimization. In <em>2016 IEEE 16th International Conference on Data Mining (ICDM)</em> (pp. 1107-1112). IEEE.</li>
<li>Wang, Z., Hutter, F., Zoghi, M., Matheson, D., &amp; de Feitas, N. (2016). Bayesian optimization in a billion dimensions via random embeddings. <em>Journal of Artificial Intelligence Research</em>, <em>55</em>, 361-387.</li>
<li>Azimi, J., Fern, A., &amp; Fern, X. (2010). Batch bayesian optimization via simulation matching. <em>Advances in Neural Information Processing Systems</em>, <em>23</em>.</li>
<li>Azimi, J., Jalali, A., &amp; Fern, X. (2012). Hybrid batch Bayesian optimization. <em>arXiv preprint arXiv:1202.5597</em>.</li>
<li>Gong, C., Peng, J., &amp; Liu, Q. (2019, May). Quantile stein variational gradient descent for batch Bayesian optimization. In <em>International Conference on Machine Learning</em> (pp. 2347-2356). PMLR.</li>
<li>Kandasamy, K., Krishnamurthy, A., Schneider, J., &amp; Póczos, B. (2018, March). Parallelised Bayesian optimisation via Thompson sampling. In <em>International Conference on Artificial Intelligence and Statistics</em> (pp. 133-142). PMLR.</li>
</ul>
<h3 id="constrained-bayesian-optimization">Constrained Bayesian Optimization</h3>
<ul>
<li>Gardner, J. R., Kusner, M. J., Xu, Z. E., Weinberger, K. Q., &amp; Cunningham, J. P. (2014, June). Bayesian Optimization with Inequality Constraints. In ICML (Vol. 2014, pp. 937-945).</li>
<li>Gelbart, Michael A., Jasper Snoek, and Ryan P. Adams. &ldquo;Bayesian optimization with unknown constraints.&rdquo; arXiv preprint arXiv:1403.5607 (2014).
Hernández-Lobato, José Miguel, et al. &ldquo;Predictive entropy search for bayesian optimization with unknown constraints.&rdquo; International conference on machine learning. PMLR, 2015.</li>
<li>Perrone, Valerio, et al. &ldquo;Constrained Bayesian optimization with max-value entropy search.&rdquo; arXiv preprint arXiv:1910.07003 (2019).</li>
<li>Letham, Benjamin, et al. &ldquo;Constrained Bayesian optimization with noisy experiments.&rdquo; Bayesian Analysis 14.2 (2019): 495-519.</li>
<li>Eriksson, David, and Matthias Poloczek. &ldquo;Scalable constrained bayesian optimization.&rdquo; International Conference on Artificial Intelligence and Statistics. PMLR, 2021.</li>
<li>Chen, Wenjie, Shengcai Liu, and Ke Tang. &ldquo;A new knowledge gradient-based method for constrained bayesian optimization.&rdquo; arXiv preprint arXiv:2101.08743 (2021).</li>
<li>Takeno, S., Tamura, T., Shitara, K., &amp; Karasuyama, M. (2021). Sequential-and Parallel-Constrained Max-value Entropy Search via Information Lower Bound. arXiv preprint arXiv:2102.09788.</li>
</ul>
<h3 id="multi-objective-bayesian-optimization">Multi-Objective Bayesian Optimization</h3>
<ul>
<li>Belakaria, S., Deshwal, A., &amp; Doppa, J. R. &ldquo;Max-value entropy search for multi-objective bayesian optimization.&rdquo; Advances in neural information processing systems 32 (2019).</li>
<li>Belakaria, S., Deshwal, A., Jayakodi, N. K., &amp; Doppa, J. R. (2020, April). Uncertainty-aware search framework for multi-objective Bayesian optimization. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 06, pp. 10044-10052).</li>
<li>Belakaria, S., Deshwal, A., &amp; Doppa, J. R. (2020, April). Multi-fidelity multi-objective Bayesian optimization: An output space entropy search approach. In Proceedings of the AAAI Conference on artificial intelligence (Vol. 34, No. 06, pp. 10035-10043).</li>
<li>Belakaria, S., Deshwal, A., &amp; Doppa, J. R. (2021). Output space entropy search framework for multi-objective Bayesian optimization. Journal of Artificial Intelligence Research, 72, 667-715.</li>
<li>Knowles, J. (2006). ParEGO: A hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems. IEEE Transactions on Evolutionary Computation, 10(1), 50-66.</li>
<li>Emmerich, M., &amp; Klinkenberg, J. W. (2008). The computation of the expected improvement in dominated hypervolume of Pareto front approximations. Rapport technique, Leiden University, 34, 7-3.</li>
<li>Zuluaga, M., Sergent, G., Krause, A., &amp; Püschel, M. (2013, February). Active learning for multi-objective optimization. In International Conference on Machine Learning (pp. 462-470). PMLR.</li>
<li>Picheny, V. (2015). Multiobjective optimization using Gaussian process emulators via stepwise uncertainty reduction. Statistics and Computing, 25(6), 1265-1280.</li>
<li>Hernández-Lobato, D., Hernandez-Lobato, J., Shah, A., &amp; Adams, R. (2016, June). Predictive entropy search for multi-objective bayesian optimization. In International conference on machine learning (pp. 1492-1501). PMLR.</li>
<li>Shah, A., &amp; Ghahramani, Z. (2016, June). Pareto frontier learning with expensive correlated objectives. In International conference on machine learning (pp. 1919-1927). PMLR.</li>
<li>Paria, B., Kandasamy, K., &amp; Póczos, B. (2020, August). A flexible framework for multi-objective bayesian optimization using random scalarizations. In Uncertainty in Artificial Intelligence (pp. 766-776). PMLR.</li>
<li>Zhang, R., Golovin, D.. (2020). Random Hypervolume Scalarizations for Provable Multi-Objective Black Box Optimization. Proceedings of the 37th International Conference on Machine Learning, PMLR</li>
<li>Daulton, S., Balandat, M., &amp; Bakshy, E. (2020). Differentiable expected hypervolume improvement for parallel multi-objective Bayesian optimization. Advances in Neural Information Processing Systems, 33, 9851-9864.</li>
<li>Suzuki, S., Takeno, S., Tamura, T., Shitara, K., &amp; Karasuyama, M. (2020, November). Multi-objective Bayesian optimization using Pareto-frontier entropy. In International Conference on Machine Learning (pp. 9279-9288). PMLR.</li>
<li>Konakovic Lukovic, M., Tian, Y., &amp; Matusik, W. (2020). Diversity-guided multi-objective bayesian optimization with batch evaluations. Advances in Neural Information Processing Systems, 33, 17708-17720.</li>
<li>Daulton, S., Balandat, M., &amp; Bakshy, E. (2021). Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement. Advances in Neural Information Processing Systems, 34.</li>
<li>Garrido-Merchán, Eduardo C., and Daniel Hernández-Lobato. &ldquo;Predictive entropy search for multi-objective bayesian optimization with constraints.&rdquo; Neurocomputing 361 (2019): 50-68.</li>
<li>Fernández-Sánchez, D., Garrido-Merchán, E. C., &amp; Hernández-Lobato, D. (2020). Improved Max-value Entropy Search for Multi-objective Bayesian Optimization with Constraints. arXiv preprint arXiv:2011.01150.</li>
<li>Chowdhury, S. R., &amp; Gopalan, A. (2021, March). No-regret algorithms for multi-task bayesian optimization. In International Conference on Artificial Intelligence and Statistics (pp. 1873-1881). PMLR.</li>
</ul>
<h3 id="bo-over-conditionaltree-structured-search-space">BO over conditional/tree-structured search space</h3>
<ul>
<li>Jenatton, R., Archambeau, C., González, J., &amp; Seeger, M. (2017, July). Bayesian optimization with tree-structured dependencies. In <em>International Conference on Machine Learning</em> (pp. 1655-1664). PMLR.</li>
<li>Swersky, K., Duvenaud, D., Snoek, J., Hutter, F., &amp; Osborne, M. A. (2014). Raiders of the lost architecture: Kernels for Bayesian optimization in conditional parameter spaces. <em>arXiv preprint arXiv:1409.4011</em>.</li>
<li>Ma, X., &amp; Blaschko, M. (2020, June). Additive tree-structured covariance function for conditional parameter spaces in Bayesian optimization. In International Conference on Artificial Intelligence and Statistics (pp. 1015-1025). PMLR.</li>
</ul>
</div>
  

  
  

  

</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  

  

  
  






  




  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

      

    
    <script src="/js/vendor-bundle.min.b73dfaac3b6499dc997741748a7c3fe2.js"></script>

    
    
    
      
      

      
      

      

      

    

    
    
    

    
    

    
    
    

    
    

    
    
    
    

    
    

    

    
    
    
    <script id="page-data" type="application/json">{"use_headroom":true}</script>

    
    
      <script src="/js/wowchemy-headroom.6803974b207c4147f10c7f9a19448266.js" type="module"></script>
    
    
    
    
    
    
    
    
    <script src="/en/js/wowchemy.min.0879156d664caf1dd0b240fa1b246a30.js"></script>

    
    
    
    
    
    
      
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      <script src="/js/wowchemy-publication.ee00aa4e09ee62617fe2dc15bfcb3f7b.js" type="module"></script>






</body>
</html>
